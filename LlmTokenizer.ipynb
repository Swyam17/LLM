{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LN69jcb3PNhL5e5ZILhJvppxBNVlPkPR",
      "authorship_tag": "ABX9TyMb7K6wu0z2xVYB4SSJHdNi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swyam17/LLM/blob/main/LlmTokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**S1**: CREATING TOKENS"
      ],
      "metadata": {
        "id": "M_hhTfwfEhHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2e2y7iq3fXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e638093-714f-4b6f-82a5-00b27a0495ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20781\n",
            "THE VERDICT\n",
            "June 1908\n",
            "\n",
            "I had always thought Jack Gisburn rather a cheap genius--though a\n",
            "\n",
            "good fell\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. This , is a test.\"\n",
        "result = re.split(r'(\\s)',text) # this is used to split the text and even the blank spaces\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rTGcBxR8b_U",
        "outputId": "c5274079-5ef5-4c9e-b645-08310d327474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This', ' ', ',', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. This , is a test.\"\n",
        "result = re.split(r'([,.]|\\s)',text) # couma is seperate token\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uTrh4D-9AGH",
        "outputId": "b20e4ea4-7154-458a-e131-af6131cf35f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ' ', '', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()] #removes blank space\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIxbvVhw_4WO",
        "outputId": "d6418869-ba12-483b-fc9a-81d7ceb03e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVING WHITESPACES OR NOT\n",
        "\n",
        "\n",
        "When developing a simple tokenizer, whether we should encode whitespaces as\n",
        "separate characters or just remove them depends on our application and its\n",
        "requirements. Removing whitespaces reduces the memory and computing\n",
        "requirements. However, keeping whitespaces can be useful if we train models that\n",
        "are sensitive to the exact structure of the text (for example, Python code, which is\n",
        "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity\n",
        "and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme\n",
        "that includes whitespaces."
      ],
      "metadata": {
        "id": "rxYAZRF3AWdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION\n",
        "\n",
        "The tokenization scheme we devised above works well on the simple sample text. Let's\n",
        "modify it a bit further so that it can also handle other types of punctuation, such as\n",
        "question marks, quotation marks, and the double-dashes we have seen earlier in the first\n",
        "100 characters of Edith Wharton's short story, along with additional special characters:"
      ],
      "metadata": {
        "id": "rYXvUkgbCRat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "result = [item for item in result if item.strip()] #removes blank space\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSnkGtfPCfjO",
        "outputId": "f3db6523-6229-4dd5-adec-402e2958c242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrNepJIVLPfm",
        "outputId": "2f2d14a0-faee-41c0-a87c-e29b6484db1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU4XAOa9DfUx",
        "outputId": "b99f3549-58fe-4574-d9ea-07f88de54876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['THE', 'VERDICT', 'June', '1908', 'I', 'had', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0qbGC0cEYic",
        "outputId": "dcbde97f-2136-4ec3-b6d1-2e0cf9437e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**S2**: CREATING TOKEN IDS"
      ],
      "metadata": {
        "id": "3FwHYtbiEetN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed)) #sort the words\n",
        "vocab_size = len(all_words) #length of words\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "tztZf74DFvmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78942b6b-7ce4-415c-9160-7b1ca46bd4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab ={token: integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "3yuo3yCql20X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i>=50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk3hCXQgTIwF",
        "outputId": "2bab3885-26ab-43c1-f882-4cfb6211d262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "('1908', 8)\n",
            "(':', 9)\n",
            "(';', 10)\n",
            "('?', 11)\n",
            "('A', 12)\n",
            "('AM', 13)\n",
            "('Ah', 14)\n",
            "('Among', 15)\n",
            "('And', 16)\n",
            "('Are', 17)\n",
            "('Arrt', 18)\n",
            "('As', 19)\n",
            "('At', 20)\n",
            "('Be', 21)\n",
            "('Begin', 22)\n",
            "('Burlington', 23)\n",
            "('But', 24)\n",
            "('By', 25)\n",
            "('Carlo', 26)\n",
            "('Chicago', 27)\n",
            "('Claude', 28)\n",
            "('Come', 29)\n",
            "('Croft', 30)\n",
            "('Destroyed', 31)\n",
            "('Devonshire', 32)\n",
            "('Don', 33)\n",
            "('Dubarry', 34)\n",
            "('Emperors', 35)\n",
            "('End', 36)\n",
            "('FELT', 37)\n",
            "('Florence', 38)\n",
            "('For', 39)\n",
            "('Gallery', 40)\n",
            "('Gideon', 41)\n",
            "('Gisburn', 42)\n",
            "('Gisburns', 43)\n",
            "('Grafton', 44)\n",
            "('Greek', 45)\n",
            "('Grindle', 46)\n",
            "('Grindles', 47)\n",
            "('HAD', 48)\n",
            "('HAS', 49)\n",
            "('HAVE', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECODING  \n",
        "\n",
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "    \n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "r0gUUlZDTuqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenzerV1:\n",
        "  #init takes vocab\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:?_!\"()\\']|--|\\s)',text)\n",
        "    #s=token i-token id\n",
        "    #white spaces remove\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \"\".join ([self.int_to_str[i] for i in ids])\n",
        "    #getting rid of spaces before the punctuations\n",
        "    text = re.sub(r'\\s +([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Su2NuTMpW5Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert token to token id"
      ],
      "metadata": {
        "id": "e3RWar0nUiBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenzerV1(vocab)\n",
        "text =\"\"\"\"It's the last he painted, you know,\"\n",
        "          Mrs.Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGFwXNZoO9kl",
        "outputId": "70208088-3eba-4131-daf9-a6d8ff127108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 62, 2, 868, 1006, 619, 550, 764, 5, 1144, 614, 5, 1, 76, 7, 42, 869, 1126, 772, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "back to token"
      ],
      "metadata": {
        "id": "4BS93QyiUoZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dtKV8FGIUgWo",
        "outputId": "d3d0da35-0988-414a-a7a1-95db0d45dd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"It\\'sthelasthepainted,youknow,\"Mrs.Gisburnsaidwithpardonablepride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADDING SPECIAL TOKENS\n",
        "\n",
        "WE NEED IT BECAUSE THERE MAY BE WORDS WHICH ARE NOT THERE IN THE VACOB SO TO HANDLE THOSE WORDS WE NEED IT\n",
        "\n",
        "we use SimpleTokenV2, to support two new tokens, <|unk|> and <|endoftext|>"
      ],
      "metadata": {
        "id": "Fdkg27WCVK48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "zjUH7frcU4f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55U3U5mRa9WG",
        "outputId": "5b5729ed-8766-454e-c7c3-8042aa3b5b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1150"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocabulary size is increased by 2, earlier it was 1148 now 1150"
      ],
      "metadata": {
        "id": "QcmkmDppbTFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print out the last 5 entries of updated vocabulary\n",
        "for i, item in enumerate (list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82la5IHLbMZG",
        "outputId": "62c04a76-3217-4923-8a84-26dfc1d4516a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1145)\n",
            "('your', 1146)\n",
            "('yourself', 1147)\n",
            "('<|unk|>', 1148)\n",
            "('<|endoftext|>', 1149)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenzerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:?_!\"()\\']|--|\\s)',text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int\n",
        "        else \"<|unk|>\" for item in preprocessed # if text is not there in vocab then it is replaced by unkown token\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join ([self.int_to_str[i] for i in ids]) # Join with space\n",
        "    # Getting rid of spaces before the punctuations and double-dash\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\u0027]|--)', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "mp24zeaUcF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenzerV2(vocab)\n",
        "text1 =\"Swyam, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of palace.\"\n",
        "\n",
        "text = \"<|endoftext|> \" .join([text1, text2])\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NKOJ1JLdjLG",
        "outputId": "a6c138cb-7d31-413f-db6b-6ac634196a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swyam, do you like tea?<|endoftext|> In the sunlit terraces of palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYVZzYgveTPC",
        "outputId": "5ab1e2e0-e4ac-4f14-f643-be605b9f2fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1148, 5, 372, 1144, 645, 993, 11, 1149, 61, 1006, 974, 1002, 740, 1148, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "44dc98b9",
        "outputId": "c18cffab-3b0d-4914-ff85-b297ebc816d9"
      },
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "other tokens:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to <|endoftext|>. For instance, when combining two different\n",
        "Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "[PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "rmWKr-zGhNAX"
      }
    }
  ]
}